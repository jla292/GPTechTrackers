# -*- coding: utf-8 -*-
"""NvidiaTradingVolume.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DPEp-hwc2utbflj3G4KxjTa3vZCuf4P9
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates
import datetime

# Define the LSTM model with dropout
class LSTMStockPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.2):
        super(LSTMStockPredictor, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Define a dataset class
class StockDataset(Dataset):
    def __init__(self, data, target, transform=None):
        self.data = data
        self.target = target
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        sample = {'data': self.data[idx], 'target': self.target[idx]}

        if self.transform:
            sample = self.transform(sample)

        return sample

# Load and preprocess data
def load_data(file_path):
    # Load data from CSV file
    df = pd.read_csv(file_path)

    # Feature selection (stock price and trading volume)
    selected_features = ['Date', 'Adj Close', 'Volume']
    df = df[selected_features]

    # Convert 'Date' column to datetime
    df['Date'] = pd.to_datetime(df['Date'])

    # Normalize the data
    scaler = MinMaxScaler()
    df_scaled = scaler.fit_transform(df[['Adj Close']])  # Only fit scaler to closing prices
    df[['Adj Close']] = df_scaled  # Update DataFrame with scaled closing prices

    return df, scaler

# Prepare data for training and testing
def prepare_data(df, sequence_length):
    X, y = [], []
    for i in range(len(df) - sequence_length):
        X.append(df.iloc[i:i+sequence_length, 1:].values)
        y.append(df.iloc[i+sequence_length, 1])
    return np.array(X), np.array(y)

def train_model(model, train_loader, criterion, optimizer, num_epochs, test_loader):
    train_losses = []
    val_losses = []
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
            inputs, labels = batch['data'].float(), batch['target'].float()
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels.unsqueeze(1))
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        train_losses.append(running_loss / len(train_loader))

        # Validation loss
        val_loss = evaluate_model(model, test_loader, criterion)
        val_losses.append(val_loss)

        print(f"Epoch {epoch + 1}, Training Loss: {train_losses[-1]}, Validation Loss: {val_loss}")

    return train_losses, val_losses

# Evaluate the model
def evaluate_model(model, test_loader, criterion):
    model.eval()
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluation"):
            inputs, labels = batch['data'].float(), batch['target'].float()
            outputs = model(inputs)
            loss = criterion(outputs, labels.unsqueeze(1))
    return loss.item()

# Train the model with L2 regularization
def train_model_with_regularization(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience):
    best_val_loss = float('inf')
    patience_count = 0
    train_losses, val_losses = [], []

    for epoch in range(num_epochs):
        model.train()
        running_train_loss = 0.0
        for batch in train_loader:
            inputs, labels = batch['data'].float(), batch['target'].float()
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels.unsqueeze(1))
            # L2 regularization
            l2_reg = None
            for param in model.parameters():
                if l2_reg is None:
                    l2_reg = param.norm(2)
                else:
                    l2_reg = l2_reg + param.norm(2)
            loss += 0.001 * l2_reg
            loss.backward()
            optimizer.step()
            running_train_loss += loss.item()
        train_loss = running_train_loss / len(train_loader)
        train_losses.append(train_loss)

        # Evaluate on validation set
        val_loss = evaluate_model(model, val_loader, criterion)
        val_losses.append(val_loss)

        print(f"Epoch {epoch + 1}, Train Loss: {train_loss}, Val Loss: {val_loss}")

        # Check for early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_count = 0
        else:
            patience_count += 1
            if patience_count >= patience:
                print("Early stopping triggered.")
                break

        # Adjust learning rate
        scheduler.step(val_loss)

    return train_losses, val_losses

def plot_predictions_with_dates(predicted_values, actual_values, dates_2024, historical_dates, historical_prices, actual_dates_2024, actual_prices_2024):
    plt.figure(figsize=(12, 8))

    # Plot historical and actual closing prices (2023 to April 2024)
    plt.plot(historical_dates, historical_prices, label='Historical & Actual (2023 - April 2024)', marker='o', color='green')
    # Plot actual closing prices (January 2024 to April 2024)
    plt.plot(actual_dates_2024, actual_prices_2024, marker='o', color='green')  # Plot without label to include in the same line
    # Plot predicted closing prices (January 2024 to April 2024)
    plt.plot(dates_2024, predicted_values, label='Predicted (January 2024 - April 2024)', marker='o', color='orange')

    plt.title('NVDA Predicted vs. Actual Stock Prices')
    plt.xlabel('Date')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()

    plt.show()

def print_evaluation_metrics(mae, rmse, actual_values_scaled):
    # Convert MAE and RMSE to percentage of error
    mae_percentage = (mae / np.mean(actual_values_scaled)) * 100
    rmse_percentage = (rmse / np.mean(actual_values_scaled)) * 100

    # Print MAE and RMSE as percentage of error
    print("Mean Absolute Error (MAE):", mae)
    print("Root Mean Squared Error (RMSE):", rmse)
    print("Mean Absolute Error (MAE) Percentage:", mae_percentage, "%")
    print("Root Mean Squared Error (RMSE) Percentage:", rmse_percentage, "%")

def plot_loss(train_losses, val_losses):
    epochs = range(1, len(train_losses) + 1)

    plt.plot(epochs, train_losses, label='Training Loss')
    plt.plot(epochs, val_losses, label='Validation Loss')

    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

def main(patience=3):
    # Load and preprocess data
    df, scaler = load_data("data/NVDA.csv")

    # Define sequence length
    sequence_length = 10

    # Split data into training (2023) and testing (first 3 months of 2024)
    train_df = df[df['Date'].dt.year == 2023]
    test_df = df[(df['Date'].dt.year == 2024) & (df['Date'].dt.month <= 3)]

    # Prepare data
    X_train, y_train = prepare_data(train_df, sequence_length)
    X_test, y_test = prepare_data(test_df, sequence_length)

    # Create datasets and dataloaders
    train_dataset = StockDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_dataset = StockDataset(X_test, y_test)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # Define model, loss function, and optimizer
    model = LSTMStockPredictor(input_size=X_train.shape[2], hidden_size=64)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Split training data into training and validation sets
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, verbose=True)

    # Train the model and capture losses
    num_epochs = 50
    train_losses, val_losses = train_model_with_regularization(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience)

    # Plot training and validation loss
    plot_loss(train_losses, val_losses)

    # Evaluate the model on the test set
    test_loss = evaluate_model(model, test_loader, criterion)
    print(f"Test Loss: {test_loss}")

    # Predict using the trained model
    model.eval()
    with torch.no_grad():
        inputs = torch.tensor(X_test).float()
        outputs = model(inputs)

        # Extract only the closing prices for actual values
        actual_values_scaled = scaler.inverse_transform(y_test.reshape(-1, 1))

        # Inverse transform the predicted values
        predicted_values_scaled = np.concatenate((X_test[:, -1, :], outputs.numpy()), axis=1)[:, 0].reshape(-1, 1)

        # Predicted values here
        predicted_values = scaler.inverse_transform(predicted_values_scaled)

    # Calculate MAE and RMSE
    mae = np.mean(np.abs(actual_values_scaled - predicted_values))
    rmse = np.sqrt(np.mean((actual_values_scaled - predicted_values) ** 2))

    # Print evaluation metrics
    print_evaluation_metrics(mae, rmse, actual_values_scaled)

    # Get historical dates and prices for 2023
    historical_dates_2023 = train_df['Date']
    historical_prices_2023 = scaler.inverse_transform(train_df[['Adj Close']])

    # Get dates for predicted values (first three months of 2024)
    dates_2024 = test_df['Date'][-len(predicted_values):]

    # Get actual dates and prices for 2024
    actual_dates_2024 = test_df['Date']
    actual_prices_2024 = scaler.inverse_transform(test_df[['Adj Close']])

    # Plot predicted vs. actual values with dates (2023 and 2024)
    plot_predictions_with_dates(predicted_values[:, 0], actual_values_scaled[:, 0], dates_2024, historical_dates_2023, historical_prices_2023, actual_dates_2024, actual_prices_2024)

    return predicted_values

def get_predictions():
    return main()

if __name__ == "__main__":
    main()