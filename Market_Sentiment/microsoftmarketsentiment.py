# -*- coding: utf-8 -*-
"""NvidiaMarketSentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PdwgqLNsFiGy3-BY__xXX_K7eN-RlmQm
"""

import torch.nn as nn
import torch
from tqdm import tqdm
import os
import pandas as pd
import torchtext
from torchtext.vocab import Vocab
from torchtext.data.utils import get_tokenizer
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
import torch.nn.utils.rnn as rnn_utils
import matplotlib.pyplot as plt
import numpy as np
from transformers import pipeline
import torch.optim as optim
import torch
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestRegressor

# done?
class LSTM(nn.Module):
    def __init__(self, input_dim, embedding_dim, hid_dim, final_dim, batch_size):
        super(LSTM, self).__init__()

        self.batch_size = batch_size
        self.hid_dim = hid_dim
        self.embedding_layer = nn.Embedding(input_dim, embedding_dim)
        # self.lstm_layer = nn.LSTM(input_size=embedding_dim, hidden_size=hid_dim, batch_first=True, bidirectional=False, num_layers=1)
        self.lstm_cell = nn.LSTMCell(input_size=embedding_dim, hidden_size=hid_dim)
        self.linear_proj = nn.Linear(2 * hid_dim, hid_dim)
        self.output_layer = nn.Linear(hid_dim, final_dim)
        self.softmax = nn.Softmax(dim=1)
        self.act = nn.ReLU()

    def forward(self, input):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        sent_len = []
        for sent in input:
          i = 0
          while i < len(sent) and sent[i] != 0:
            i += 1
          sent_len.append(i-1)

        embeds = self.embedding_layer(input)
        batch_size = input.size(0)
        h_0, c_0 = torch.randn(batch_size, self.hid_dim).to(device), torch.randn(batch_size, self.hid_dim).to(device)
        hid_states = []
        cell_states = []
        for i in range(input.shape[1]):
            h_0, c_0 = self.lstm_cell(embeds[:, i, :], (h_0, c_0))
            hid_states.append(h_0)
            cell_states.append(c_0)

        final_hid_states = []
        for s in range(len(sent_len)):
          last_hid = hid_states[sent_len[s]][s]
          last_cell = cell_states[sent_len[s]][s]
          final_hid_states.append(torch.cat((last_hid, last_cell)))
        final_hid_states = torch.stack(final_hid_states).to(device)

        x = self.linear_proj(final_hid_states)
        x = self.output_layer(x)
        # x = self.softmax(x)
        return x

# plan: read from csv to df, split into train/test/val, tokenize & pad, return all with vocab size
def format_dataloader(file_path_train, file_path_test):
    # load data
    df1 = pd.read_csv(file_path_train, engine='python')
    df2 = pd.read_csv(file_path_test, engine='python')

    X_train = df1["Titles"] + " " + df1["Descriptions"] # include both title and then description
    y_train = df1["Label"] # label encoding: 0 as bullish, 1 as bearish, 2 as neutral
    X_test = df2["Titles"] + " " + df2["Descriptions"] # include both title and then description
    y_test = df2["Labels"] # label encoding: 0 as bullish, 1 as bearish, 2 as neutral

    # split into test and val data
    third = int(len(X_test) / 3)
    X_val = X_test.head(third)
    y_val = y_test.head(third)
    X_test = X_test.tail(third * 2).reset_index(drop=True)
    y_test = y_test.tail(third *2).reset_index(drop=True)

    # tokenize & pad continue
    tokenizer = torchtext.data.utils.get_tokenizer("basic_english")
    tokenized_data_train = [tokenizer(sentence) for sentence in X_train]
    unique_tokens = set(token for seq in tokenized_data_train for token in seq)
    vocab = {token: idx for idx, token in enumerate(unique_tokens)}

    X_train = [torch.tensor([vocab[token] for token in tokenized_sentence]) for tokenized_sentence in tokenized_data_train]
    # X_train_final = [len(tokenized_sentence) - 1 for tokenized_sentence in tokenized_data_train]
    vocab['<unk>'] = len(vocab)
    X_train = [torch.tensor([vocab.get(token, vocab['<unk>']) for token in tokenized_sentence]) for tokenized_sentence in tokenized_data_train]
    X_train_len = torch.tensor([len(tokenized_sentence) - 1 for tokenized_sentence in tokenized_data_train])
    X_train = rnn_utils.pad_sequence(X_train, batch_first=True)

    max_seq_length_train = max(len(seq) for seq in X_train)
    padded_data_train = rnn_utils.pad_sequence([torch.tensor(seq) for seq in X_train], batch_first=True)

    # input size
    vocab_size = len(vocab)

    # tokenize & pad continue
    tokenized_data_test = [tokenizer(sentence) for sentence in X_test]
    X_test = [torch.tensor([vocab.get(token, vocab['<unk>']) for token in tokenized_sentence]) for tokenized_sentence in tokenized_data_test]
    X_test_len = torch.tensor([len(tokenized_sentence) - 1 for tokenized_sentence in tokenized_data_test])
    X_test = rnn_utils.pad_sequence(X_test, batch_first=True)

    tokenized_data_val = [tokenizer(sentence) for sentence in X_val]
    X_val = [torch.tensor([vocab.get(token, vocab['<unk>']) for token in tokenized_sentence]) for tokenized_sentence in tokenized_data_val]
    X_val_len = torch.tensor([len(tokenized_sentence) - 1 for tokenized_sentence in tokenized_data_val])
    X_val = rnn_utils.pad_sequence(X_val, batch_first=True)

    return X_train, y_train, X_test, y_test, X_val, y_val, vocab_size, X_train_len, X_test_len, X_val_len

def train_model(
    model, train, val, num_epochs, optimizer, loss_fn, batch_size=32, file=None
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        print(f"Epoch {epoch}")
        total_loss = 0
        running_loss = 0.0
        model.train()
        total_batchs = 0
        for batch in tqdm(train, desc="Training Batches"):
            optimizer.zero_grad()
            input = batch[0].to(device)
            label = batch[1].to(device)

            output = model(input)
            # print(output)
            # print(label)
            # assert False
            loss = loss_fn(output, label)
            total_loss += loss.item()
            total_batchs += 1

            running_loss += loss.item()
            loss.backward()
            optimizer.step()

        epoch_loss = running_loss / total_batchs
        train_losses.append(epoch_loss)
        print(f"Training Loss: {total_loss / total_batchs}")

        val_loss = 0
        running_loss = 0.0
        total_batchs = 0
        model.eval()
        for batch in tqdm(val, desc="validation batches"):
            input = batch[0].to(device)
            label = batch[1].to(device)

            output = model(input)
            loss = loss_fn(output, label)
            val_loss += loss.item()
            running_loss += loss.item()
            total_batchs += 1
        epoch_loss = running_loss / len(train)
        val_losses.append(epoch_loss)

        print(f"Validation Loss: {val_loss / total_batchs}")
    if file:
        path = os.path.join(os.getcwd(), "models", file)
        model.save_pretrained(path)

    return num_epochs, train_losses, val_losses

def eval_model(model, test, loss_fn):
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.eval()
  test_loss = 0
  total_batchs = 0
  outputs = []
  labels = []
  for batch in tqdm(test, desc="test batches"):
      input = batch[0].to(device)
      label = batch[1].to(device)

      output = model(input)

      loss = loss_fn(output, label)
      test_loss += loss.item()
      total_batchs += 1
      outputs.append(output)
      labels.append(label)
  print(f"Test Loss: {test_loss / total_batchs}")
  return outputs, labels, test_loss / total_batchs

def pred_model(model, input):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    input = input.to(device)

    output = model(input)
    return output

def save_model(model, path):
  torch.save(model, path)

def load_model(path):
  return torch.load(path)


train_path = os.path.join(os.getcwd(), "data", "microsoft_train.csv")
test_path = os.path.join(os.getcwd(), "data", "microsoft_test.csv")
reg_path = os.path.join(os.getcwd(), "data", "microsoft_stock.csv")

# parameters
embedding_dim = 64
hid_dim = 64
final_dim = 3
max_length = 150 #300
lr = 0.001 #0.001
num_epochs = 5
batch_size= 16

# Initialize sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

# load data
X_train, y_train, X_test, y_test, X_val, y_val, vocab_size, X_train_len, X_test_len, X_val_len = format_dataloader(train_path, test_path)

X_train = X_train[:1360] #1216
y_train = y_train[:1360]
train_dataset = TensorDataset(X_train, torch.tensor(y_train))
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = TensorDataset(X_test, torch.tensor(y_test))
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)
val_dataset = TensorDataset(X_val, torch.tensor(y_val))
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

# get input size
input_dim = vocab_size

# other definitions
# models = []
loss_fn = torch.nn.CrossEntropyLoss() # CrossEntropyLoss
# loss_fn = torch.nn.NLLLoss() # NLLLoss
# preds = [0.0 for _ in range(10)]

# model
for n in range(1):
    model = LSTM(input_dim, embedding_dim, hid_dim, final_dim, batch_size)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    num_epochs, train_losses, val_losses = train_model(model, train_dataloader, val_dataloader, num_epochs, optimizer, loss_fn)

    # Plotting the training loss over epochs
    plt.plot(range(1, num_epochs + 1), train_losses, color="red", label='Training Loss')
    plt.plot(range(1, num_epochs + 1), val_losses, color="blue", label='Validation Loss')
    plt.grid()
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    # Get the current working directory
    current_dir = os.getcwd()

    # Define the path to save the model
    model_save_path = os.path.join(current_dir, f"microsoft_lstm_{n}.pth")

    # Save the model
    save_model(model, model_save_path)

sentiment_model_path = os.path.join(current_dir, "microsoft_lstm_0.pth")
sentiment_model = load_model(sentiment_model_path)

# Generate sentiment predictions for each article in the financial data
sentiment_predictions = []
for article in X_train:
    sentiment_label = pred_model(sentiment_model, article.unsqueeze(0))  # Unsqueeze to add batch dimension
    sentiment_predictions.append(torch.argmax(sentiment_label).item())

financial_data = pd.read_csv(reg_path)
financial_data = financial_data.dropna()

# Ensure the length of sentiment predictions matches the length of financial data
num_articles = len(financial_data)
sentiment_predictions = sentiment_predictions[:num_articles]

# Add sentiment predictions to financial data
financial_data["Sentiment"] = sentiment_predictions

# Extract features and target variable for the prediction model
X = financial_data.drop(columns=["Date", "Close", "Adj Close"])  # Features
y = financial_data["Adj Close"]  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = RandomForestRegressor(random_state=42)

# Fit the model to the training data
model.fit(X_train, y_train)

# Evaluate the model
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)

print(f"Training R^2 Score: {train_score}")
print(f"Testing R^2 Score: {test_score}")

# Make predictions
predictions = model.predict(X_test)

plt.figure(figsize=(10, 6))
plt.plot(y_test.values, label='Actual Prices', color='blue')
plt.plot(predictions, label='Predicted Prices', color='red')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.title('Actual vs Predicted Stock Prices')
plt.legend()
plt.grid(True)
plt.show()
# Plot actual and predicted stock prices against approximate dates for sentiment data
plt.figure(figsize=(10, 6))
num_train_samples = len(X_train)
num_test_samples = len(X_test)

# Generate approximate dates for the sentiment training data
start_date = pd.to_datetime("2023-01-01")
end_date_train = start_date + pd.Timedelta(days=num_train_samples - 1)  # Adjusted end date calculation
train_dates = pd.date_range(start=start_date, end=end_date_train, freq='D')

# Convert dates to strings for plotting
train_dates_str = train_dates.strftime('%Y-%m-%d')

# Plot actual prices against approximate dates for sentiment training data
plt.plot(train_dates_str, y_train.values, label='Actual Prices (Train)', color='green', linestyle='-')

# Generate approximate dates for the sentiment test data
start_date_test = end_date_train + pd.Timedelta(days=1)
end_date_test = start_date_test + pd.Timedelta(days=num_test_samples - 1)  # Adjusted end date calculation
test_dates = pd.date_range(start=start_date_test, end=end_date_test, freq='D')

# Convert test dates to strings for plotting
test_dates_str = test_dates.strftime('%Y-%m-%d')

# Plot actual prices against actual dates for sentiment test data
plt.plot(test_dates_str, y_test.values, label='Actual Prices (Test)', color='red', linestyle='-')

# Plot predicted prices against actual dates for sentiment test data
plt.plot(test_dates_str, predictions, label='Predicted Prices', color='blue')

plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.title('Actual vs Predicted Stock Prices')
plt.legend()
plt.grid(True)

# Set x-axis ticks at monthly intervals
all_dates_str = np.concatenate((train_dates_str, test_dates_str))
plt.xticks(all_dates_str[::30], rotation=45, ha='right')

plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

# Print the accuracy between the predicted and the test data
test_accuracy = model.score(X_test, y_test)
print(f"Accuracy between predicted and test data: {test_accuracy}")